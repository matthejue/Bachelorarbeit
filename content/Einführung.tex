%!Tex Root = ../Main.tex
% ./Packete_und_Deklarationen.tex
\chapter{Einführung}
\label{ch:einführung}

\section{Compiler und Interpreter}
\begin{Definition}{Compiler}{compiler}
\end{Definition}
\begin{Definition}{Interpreter}{Interpreter}
% TODO: Bild semantisch gleiche Bedeutung
\end{Definition}
\subsection{T-Diagramme}
\begin{Definition}{T-Diagram}{t_diagram}
\end{Definition}
\section{Grammatiken}
\section{Grundlagen}
\begin{Definition}{Sprache}{Sprache}
\end{Definition}
\begin{Definition}{Chromsky Hierarchie}{chromsky_hierarchie}
\end{Definition}
\begin{Definition}{Grammatik}{grammatik}
\end{Definition}
\begin{Definition}{Reguläre Sprachen}{reguläre_sprachen}
\end{Definition}
\begin{Definition}{Kontextfreie Sprachen}{kontextfreie_sprachen}
\end{Definition}
\subsection{Mehrdeutige Grammatiken}
\begin{Definition}{Ableitungsbaum}{ableitungsbaum}
% TODO: Bild hierfür
\end{Definition}
\begin{Definition}{Mehrdeutige Grammatik}{mehrdeutige_grammatik}
% TODO: (Bild hierfür)
\end{Definition}
\subsection{Präzidenz und Assoziativität}
\begin{Definition}{Assoziativität}{assoziativität}
\end{Definition}
\begin{Definition}{Präzidenz}{präzidenz}
\end{Definition}
% \subsection{Linksrekursiv und Rechtrekursiv}
\section{Lexikalische Analyse}
\label{sec:lexikalische_analyse}

Die \colorbold{Lexikalische Analyse} bildet üblicherweise die erste Ebene innerhalb der \colorbold{Pipe Architektur} bei der Implementierung von Compilern. Die Aufgabe der lexikalischen Analyse ist vereinfacht gesagt, in einem Inputstring, z.B. dem Inhalt einer Datei, welche in \colorbold{UTF-8} codiert ist, Folgen endlicher Symbole (auch \colorbold{Wörter} genannt) zu finden, die bestimmte \colorbold{Pattern} (Definition \ref{def:pattern}) matchen, die durch eine \colorbold{reguläre Grammatik} spezifiziert sind.

\begin{Definition}{Pattern}{pattern}
  \colorbold{Beschreibung} aller möglichen \colorbold{Lexeme} einer Menge $\mathbb{P}_{T}$, die einem bestimmten \colorbold{Token} $T$ zugeordnet werden.
  Die Menge $\mathbb{P}_{T}$ ist eine möglicherweise unendliche Menge von \colorbold{Wörtern}, die sich mit den Regeln einer \colorbold{regulären Grammatik} ${G}_{Lex}$ einer \colorbold{regulären Sprache} ${L}_{Lex}$ beschreiben lassen \footnote{Als Beschreibungswerkzeug können aber auch z.B. reguläre Ausdrücke hergenommen werden.}, die für die Beschreibung eines \colorbold{Tokens} $T$ zuständig sind.\footcite{noauthor_what_nodate}
\end{Definition}

Diese Folgen endlicher Symoble werden auch \colorbold{Lexeme} (Definition \ref{def:lexeme}) genannt.

\begin{Definition}{Lexeme}{lexeme}
  Ein \colorbold{Lexeme} ist ein \colorbold{Wort} aus dem Inputstring, welches das \colorbold{Pattern} für eines der \colorbold{Token} $T$ einer \colorbold{Sprache} ${L}_{Lex}$ matched.
\footcite{noauthor_what_nodate}
\end{Definition}

Diese \colorbold{Lexeme} werden vom \colorbold{Lexer} im \colorbold{Inputstring} identifziert und \colorbold{Tokens} $T$ zugeordnet (Definition \ref{def:lexer}). Die \colorbold{Tokens} sind es, die letztendlich an die \colorbold{Syntaktische Analyse} weitergegeben werden.

\begin{Definition}{Lexer (bzw. Scanner)}{lexer}
  Ein \colorbold{Lexer} ist eine \colorbold{partielle} Funktion \hspace{0.2cm}$lex: \Sigma^{*} \rightharpoonup (N \times W)^{*}$, welche ein \colorbold{Wort} aus $\Sigma^{*}$ auf ein \colorbold{Token} $T$ mit einem \colorbold{Tokennamen} $N$ und einem \colorbold{Tokenwert} $W$ abbildet, falls diese Folge von Symbolen sich unter der \colorbold{regulären Grammatik} ${G}_{Lex}$, der \colorbold{regulären Sprache} ${L_{Lex}}$ abbleiten lässt.\footcite{noauthor_lecture-notes-2021_2022}
\end{Definition}

Ein \colorbold{Lexer} ist im Allgemeinen eine \colorbold{partielle Funktion}, da es Zeichenfolgen geben kann, die kein \colorbold{Pattern} eines \colorbold{Tokens} der Sprache $L_{Lex}$ matchen. In Bezug auf eine Implementierung, wird, wenn der Lexer Teil der Implementierung eines Compilers ist, in diesem Fall eine \colorbold{Fehlermeldung} ausgegeben.

Eine weitere Aufgabe der \colorbold{Lekikalischen Analyse} ist es jegliche für die Weiterverarbeitung unwichtigen Symbole, wie Leerzeichen \,\textvisiblespace\,, Newline \verb|\n|\footnote{In Unix Systemen wird für Newline das ASCII Symbol \colorbold{line feed}, in Windows hingegen die ASCII Symbole \colorbold{carriage return} und \colorbold{line feed} nacheinander verwendet. Das wird aber meist durch die verwendete Porgrammiersprache, die man zur Inplementierung des Lexers nutzt wegabstrahiert.} und Tabs \verb|\t| aus dem Inputstring herauszufiltern. Das geschieht mittels des \colorbold{Lexers}, der allen für die \colorbold{Syntaktische Analyse} unwichtige Zeichen das leere Wort $\epsilon$ zuordnet. Das ist auch im Sinne der Definition, denn $\epsilon \in \Sigma^{*}$.

Nur das, was für die \colorbold{Syntaktische Analyse} wichtig ist, soll weiterverarbeitet werden, alles andere wird herausgefiltert.


% In den  $G_{Lex}$ Grammatiken einiger Programmiersprachen sind allerdinds alle möglichen Zeichenfolgen allein dadurch schon möglich, weil diese Programmiesprachen das Konzept eines \colorbold{Identifiers} o.ä. umsetzen, der alle möglichen Zeichenfolgen abfängt\footnote{Bei der Grammatik von C und auch PicoC ist das allerdings nicht der Fall, weil Identifier dort nicht mit einer Zahl anfangen dürfen.}. Wodurch der Lexer wiederum doch eine linkstotale partielle Funktion ist, die man im Allgemeinen einfach als \colorbold{Funktion} bezeichnet: $lex: \Sigma^{*} \rightarrow (N \times W)^{*}$.

Der Grund warum nicht einfach nur die \colorbold{Lexeme} an die \colorbold{Syntaktische Analyse} weitergegeben werden und der Grund für die Aufteilung des \colorbold{Tokens} in \colorbold{Tokenname} und \colorbold{Tokenwert} ist, weil z.B. die Bezeichner von Variablen, Konstanten und Funktionen beliebige Zeichenfolgen sein können, wie \smalltt{my\_fun}, \smalltt{my\_var} oder \smalltt{my\_const} und es auch viele verschiedenen Zahlen gibt, wie \smalltt{42}, \smalltt{314} oder \smalltt{12}. Die Überbegriffe bzw. Tokennamen für beliebige Bezeichner von Variablen, Konstanten und Funktionen und beliebige Zahlen sind aber trotz allem z.B. \smalltt{Zahl} und \smalltt{Bezeichner}.

Ein \colorbold{Lexeme} ist damit aber nicht das gleiche, wie der \colorbold{Tokenwert}, denn z.B. im Falle von PicoC kann z.B. der Wert $99$ durch zwei verschiedene Literale darstellt werden, einmal als ASCII-Zeichen \smalltt{'c'} und des Weiteren auch in Dezimalschreibweise als \smalltt{99}\footnote{Die Programmiersprache Python erlaubt es z.B. diesern Wert auch mit den Literalen \smalltt{0b1100011} und \smalltt{0x63} darzustellen.}. Der \colorbold{Tokenwert} ist jedoch der letztendliche Wert an sich, unabhängig von der Darstellungsform.

  Die \colorbold{Grammatik} $G_{Lex}$, die zur Beschreibung der Token $T$ einer regulären Sprache $L_{Lex}$ verwendet wird, ist üblicherweise \colorbold{regulär}, da ein typischer \colorbold{Lexer} immer nur \colorbold{ein Symbol} vorausschaut\footnote{Man nennt das auch einem \colorbold{Lookahead} von $1$}, unabhängig davon, was für Symbole davor aufgetaucht sind. Die übliche Implementierung eines \colorbold{Lexers} merkt sich nicht, was für Symbole davor aufgetaucht sind.

% TODO: später erwähnen, dass alle Regeln der Grammatik G_lex eine reguläre Form haben, was der Beweis ist

\begin{Special_Paragraph}
  Um Verwirrung verzubäugen ist es wichtig folgende Unterscheidung hervorzuheben: Wenn von \colorbold{Symbolen} die Rede ist, so werden in der \colorbold{Lexikalischen Analyse}, der \colorbold{Syntaktische Analyse} und der \colorbold{Code Generierung}, auf diesen verschiedenen Ebenen unterschiedliche Konzepte als Symbole bezeichnet.

  In der Lexikalischen Analyse sind einzelne \colorbold{Zeichen eines Zeichensatzes} die Symbole.

  In der Syntaktischen Analyse sind die \colorbold{Tokennamen} die Symbole.

  In der Code Generierung sind die \colorbold{Bezeichner von Variablen, Konstanten und Funktionnen} die Symbole\footnote{Das ist der Grund, warum die Tabelle, in der Informationen zu Identifiern gespeichert werden aus Kapitel \ref{ch:implementierung} Symboltabelle genannt wird.}.
\end{Special_Paragraph}

\begin{Definition}{Literal}{literal}
  Eine von möglicherweise vielen weiteren \colorbold{Darstellungsformen} für ein und denselben \colorbold{Wert}.
\end{Definition}

% TODO: zusammenfassendes Bild

\section{Syntaktische Analyse}

In der \colorbold{Syntaktischen Analyse} ist für einige Sprachen eine \colorbold{Kontextfreie Grammatik} $G_{Parse}$ notwendig, um die diese Sprache zu beschreiben, da viele Programmiersprachen z.B. für \colorbold{Funktionsaufrufe} \verb|fun(arg)| und \colorbold{Codeblöcke} \verb|if(1){}| syntaktische Mittel verwenden, die es notwendig machen sich zu merken wieviele öffnende Klammern \verb|'('| bzw. öffnende geschweifte Klammern \verb|'{'| es momentan gibt, die noch nicht durch eine enstsprechende schließende Klammer \verb|')'| bzw. schließende geschweifte Klammer \verb|'}'| geschlossen wurden.

% TODO: später erwähnen, dass alle Regeln der Grammatik G_parse eine kontexfreie Form haben, was der Beweis ist

Die vom \colorbold{Lexer} im Inputstring identifizierten \colorbold{Token} werden in der \colorbold{Syntaktischen Analyse} vom \colorbold{Parser} (Definition \ref{def:parser}) als \colorbold{Wegweiser} verwendet, da je nachdem, in welcher Reihenfolge die \colorbold{Token} auftauchen, dies einer anderen Ableitung nach der \colorbold{Grammatik} $G_{Parse}$ entspricht. Dabei wird in der Grammatik nach dem \colorbold{Tokennamen} unterschieden und nicht nach dem Tokenwert, da es nur von Interesse ist, ob an einer bestimmten Stelle z.B. eine \verb|Zahl| steht und nicht, welchen konkretten Wert diese \verb|Zahl| hat. Der \colorbold{Tokenwert} ist erst später in der \colorbold{Code Generierung} relevant.

Die \colorbold{Syntax}, in welcher der Inputstring aufgeschrieben ist, wird auch als \colorbold{konkrette Syntax} (Definition \ref{def:konkrette_syntax}) bezeichnet.

\begin{Definition}{Parser}{parser}
  Ein Programm, dass eine \colorbold{Eingabe} in eine für die \colorbold{Weiterverbeitung} taugliche Form bringt.
\end{Definition}

In Bezug auf Compilerbau hat ein \colorbold{Parser} meist die Aufgabe aus einem \colorbold{Inputstring} einen \colorbold{Derivation Tree} (Definition \ref{def:derivation_tree}) zu generieren.

\begin{Special_Paragraph}
  An dieser Stelle könnte möglicherweise eine Begriffsverwirrung enstehen, ob ein \colorbold{Lexer} nach der obigen Definition nicht auch ein \colorbold{Parser} ist.

  In Bezug auf Compilerbau ist ein \colorbold{Lexer} ein Teil eines Parsers und der Parser vereinigt sowohl die \colorbold{Lexikalische Analyse}, als auch einen Teil der \colorbold{Syntaktischen Analyse} in sich, aber für sich isoliert betrachtet ist ein Lexer nach Definition \ref{def:parser} ebenfalls ein Parser. Aber im Compilerbau überwiegt seine Funktionalität, dass er den Inputstring lexikalisch weiterverarbeitet, um ihn als Lexer zu bezeichnen, der Teil eines Parsers ist.
\end{Special_Paragraph}

Ein \colorbold{Parser} ist aber auch ein erweiterter \colorbold{Recognizer}, denn einmal hat der \colorbold{Parser} die Aufgabe eines \colorbold{Recognizers} (Definition \ref{def:recognizer}), nämlich zu überprüfen, ob ein Inputstring sich den Regeln der Grammatik $G_Parse$ ableiten lässt und ein \colorbold{Wort} der Sprache $L_{Parse}$ ist.

\begin{Definition}{Recognizer}{recognizer}

\end{Definition}

\begin{Definition}{Konkrette Syntax}{konkrette_syntax}
  \colorbold{Syntax} einer \colorbold{Sprache}, die durch die \colorbold{Grammatiken} $G_{Lex}$ und $G_{Parse}$ zusammengenommen beschrieben wird.

  Ein \colorbold{Programm} in seiner \colorbold{Textrepräsentation}, wie es in einer Textdatei nach den Regeln der \colorbold{Grammatiken} $G_{Lex}$ und $G_{Parse}$ abgeleitet steht, bevor man es kompiliert, ist in \colorbold{konkretter Syntax} aufgeschrieben.
\end{Definition}

\begin{Definition}{Derivation Tree (bzw. Parse Tree)}{derivation_tree}
\end{Definition}

\begin{Definition}{Abstrakte Syntax}{abstrakte_syntax}

\end{Definition}

\begin{Definition}{Abstrakte Syntax Tree}{abstrakte_syntax_tree}
\end{Definition}

\begin{Definition}{Transformer}{transformer}
\end{Definition}

\begin{Definition}{Visitor}{visitor}
\end{Definition}

% TODO: zusammenfassendes Bild
\section{Code Generierung}
\begin{Definition}{Pass}{pass}
% TODO: Bild semantisch gleiche Bedeutung
% TODO: auf T-Diagramme zurückkommen
\end{Definition}
\section{Fehlermeldungen}
\begin{Definition}{Fehlermeldung}{fehlermeldung}
\end{Definition}
% Kategorien von Fehlermeldungen
